<!doctype html>
<html lang="en">

    <head>
        <meta charset="utf-8">

        <title>Theoretical and Computational Neuroscience</title>

        <meta name="description" content="Sistema Imune: Complexidade e Simulação Baseada em Agentes">
        <meta name="author" content="Lucas Silva Simões, Paulo Roberto de Oliveira Castro">

        <meta name="apple-mobile-web-app-capable" content="yes" />
        <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

        <link rel="stylesheet" href="css/reveal.min.css">
        <link rel="stylesheet" href="css/theme/night.css" id="theme">

        <!-- For syntax highlighting -->
        <link rel="stylesheet" href="lib/css/zenburn.css">

        <!-- If the query includes 'print-pdf', use the PDF print sheet -->
        <script>
            document.write( '<link rel="stylesheet" href="css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
        </script>

        <!--[if lt IE 9]>
        <script src="lib/js/html5shiv.js"></script>
        <![endif]-->

    </head>

    <body>

        <div class="reveal">
            <div class="slides">

                <!-- Título -->
                <section>
                    <h1>Theoretical and Computational Neuroscience</h1><br/><br/>
                    <h5>Lucas S. Simões and Paulo R. O. Castro</h5>
                </section>

                <section>
                    <h2>Outline of this presentation</h2><br/><br/>
                    <ul>
                        <li><b>Introduction</b> - Why modeling the Brain?</li><br/>
                        <li><b>Section I</b> - Of Bytes and Brains</li><br/>
                        <li><b>Section II</b> - Bayes in The Brain</li><br/>
                        <li><b>Section III</b> - Information Theory and Neuroscience</li><br/>
                        <li><b>Wrapping Up</b> - Conclusions and Other Works</li>
                    </ul>
                </section>


                <!-- Intro -->
                <section>
                    <h1>Introduction</h1>
                </section>

                <section>
                	<h2>Computational Neuroscience</h2>
                	<br/><br/>
                	<b>Interdisciplinary field of science</b> that study the brain function in terms of the <b>information processing</b> properties of the structures that make up the nervous system.
                </section>

                <section>
                	<br/><br/>The <b>brain</b> is one of the most interesting <b>complex systems</b> in the universe and the <b>most efficient signal processing device</b> know.
                </section>

                <section>
                	<h2>Why modeling the Brain?</h2>
                	<br/><br/>
                	Theoretical models are used to frame hypotheses that can be directly tested by biological and psychological experiments
                </section>

                <section>
                	<h2>How?</h2>
                	<br/><br/>
                	Models must <b>capture the essential features of biological systems</b> at multiple spatial-temporal scales, from membrane currents, proteins, and chemical coupling to network oscillations, architecture, learning and memory.
                </section>

                <section>
                	<h2>Why using computers to do so?</h2>
                	<br/><br/>
                	To estimate the behavior of a system that is too complex for analytical solutions, but also becase of the similarity to the brain.
                </section>

                <section>
                	<p><img src="images/wiener.jpg" style="width: 50%;"></p>
                	"...could only be made by a team os scientists, each specialist in his own field but each possessing a throughly sound and trained acquaintance with the fields of his neighbors"
                	<p align="right"><i>- Norbert Wiener</i></p>
                </section>

                <section>
	                <h2>Lines on inquiry</h2>
	                <br/><br/>
	                <ul>
	                	<li class="fragment">Single-neuron modeling</li><br/>
	                	<li class="fragment">Sensory processing</li><br/>
	                	<li class="fragment">Memory and synaptic plasticity</li><br/>
	                	<li class="fragment">Cognition</li><br/>
	                	<li class="fragment">Consciousness</li>
	            	</ul>
                </section>

                <!-- Part I -->
                <section data-background="images/bytes.jpg">
                    <h1>Section I - Of Bytes and Brains</h1>
                </section>

                <!-- Part II -->
                <section data-background="images/bayesian.jpg">
                    <h1>Section II - Bayes in the Brain</h1>
                </section>

                <section>
                	<p><img src="images/bayes.jpg" style="width: 50%;"></p>
                	<i>Thomas Bayes</i>
                </section>

				<!-- Part III -->
                <section data-background="images/info.png">
                    <h1>Section III - <br/>Information <br/>Theory and <br/>Neuroscience</h1>
                </section>

                <section>
                	<h3>Differences from artificial systems and why using this theoretical approach</h3>
                </section>

                <section>
                	<h2>Feedback</h2>
                	<br/><br/>
                	Pathways connecting regions of higher-level brain function to regions of lower level functionality. This gives rise to top-down processing theories and selective attention.
                </section>

                <section>
                	<h2>What is Information Theory?</h2>
                	<br/><br/>
                	Information theory is a branch of applied mathematics and computer science involving the <b>quantification of information</b>.
                </section>

                <section>
                    <h2>History and Importance of Information Theory</h2>
                </section>

                <section>
                	<p><img src="images/shannon.jpg" style="width: 40%;"></p>
                	<i>Claude Elwood Shannon</i>
                </section>

                <section>
                	<h3>Parameters of a transmission:</h3>
                	<br/><br/>
                	<ul>
                		<li class="fragment">Transmission Power</li><br/>
                		<li class="fragment">Transmission Rate</li><br/>
                		<li class="fragment">Transmission Code</li>
                	</ul>
                </section>

                <section>
                	<h3>Shannon work to solve perfect transmission problem</h3>
                </section>

                <section>
                	<h3>Transmission Rate vs. Channel Capacity</h3>
                </section>

                <section>
                	<h3>Intra-Organism Communication</h3>
                	And the limitation of this presentation
                </section>

                <section>
                    <h2>Basic concepts of Information Theory</h2>
                </section>

                <section data-background="images/coding.png">
                	<h3>The importance of <b>source</b> and <b>channel</b> coding</h3>
                </section>

                <section>
                    <p><img src="images/compression.png"></p>
                </section>

                <section>
                	<h3>Compressing an 8-bit message</h3>
                	<br/><br/>
                	00110101
                </section>

                <section>
                    <span class="fragment fade-out">15 most common messages:  <b>4-bits</b></span>
                    <br/><br/>
                    <span class="fragment fade-in">Other 241 messages: 1111 + <b>8-bits</b></span>
                </section>

                <section>
                    What if we had to compress a very large number of <b>8-bits</b> messages?
                    <br/><br/>
                    <span class="fragment fade-in"><br/>\[N \rightarrow Total\]</span>
                    <span class="fragment fade-in">\[K \rightarrow Common\]</span>
                    <br/><br/>
                    <span class="fragment fade-in"><br/>Total of bits required:<br/></span>
                    <span class="fragment fade-in"><br/>\[4K + 12(N - K)\]</span>
                </section>

                <section>
                	<h4>Probability formalism</h4>
                	Sequence of messages, each having a probability density $p(X)$
                	<br/><br/>
                	\[\{x_1, x_2, x_3, ...\}\]
                	<br/><br/>
                	<span class="fragment fade-in">Assume that $15$ messages have $5\%$ of chance of ocurring each. The other $241$ have the remaining $25\%$</span>
                </section>

                <section>
                	The expected number of bits required to compress a single message can be calculated as the following:
                	<br/><br/>
                	\[\sum_i p(x_i) b(x_i)\]
                </section>

                <section>
                	<h3>Is there a better way of compressing these messages?</h3>
                	<br/><br/>
                	<span class="fragment fade-in">Concatanating and dealing with symbols</span>
                </section>

                <section>
                	<h3>Entropy</h3>
                	<br/><br/>
                	<span class="fragment fade-out">\[H(X) = -E_{p(X)}[\log p(x)]\]</span>
                	<span class="fragment fade-in">\[H(X) = -\sum_{i = 1}^n p(x_i)\log p(x_i)\]</span>
                	<br/><br/>
                	<span class="fragment fade-in">Quantifies the expected value of the <b>information contained in a message</b>, therefore providing an absolute <b>limit on the best lossless encoding</b> possible.</span>
            	</section>

            	<section>
            		<h3>Shannon Coding Theorem</h3>
            		<br/><br/>
            		As the number of symbols go to infinity, it is <i>possible</i> to compress each symbols to $H(X)$ bits on average, and it's <i>impossible</i> to do better.
            	</section>

            	<section>
                    <p><img src="images/channel.png"></p>
                </section>

                <section>
                    Binary sequence to be transmitted to another device
                    <br/><br/>
                    \[\{s_1, s_2, ...\}\]
                </section>

                <section>
                	An noisy channel introduces random errors with probability $p$
                	<br/><br/>
                	\[\{x_1, x_2, ...\} \xrightarrow{channel} \{\tilde{x}_1, \tilde{x}_2, ...\}\]
                </section>

                <section>
                	What sequence $\{x_i\}$ should be sent over the channel if the aim is to send $\{s_i\}$ reliably to the receiver?
                	<br/><br/>
                	<p><img src="images/channel.png"></p>
                </section>

                <section>
                	The message $S$ is divided in block of length $K$, each encoded to a block of length $N$
                </section>

                <section>
                	<h3>Example: parity bit</h3>
                	<br/><br/>
                	\[K = 2\]
                	\[N = 3\]
                </section>

                <section>
                	<h3>Rate of the code</h3>
                	<br/><br/>
                	\[R = \frac{K}{N}\]
                	<br/><br/>
                	<span class="fragment fade-in">If $K$ is large, a sofisticated form of redundancy can be introduced, even keeping rate $R$ constant</span>
                </section>

                <section>
                	<h3>Channel Capacity</h3>
                	<br/><br/>
                	There is a rate $C$ such that, for any rate $R < C$ and any desired error rate $\varepsilon > 0$, exists a $K$ and a block coder/decoder such that the receiver can decode each bit with error probability less that $\varepsilon$. 
                </section>

                <section>
                	<h3>Mutual Information: Intuition</h3>
                	<br/><br/>
                	Measures how much information the output provides about the input. The more reliable the channel, the larger the mutual information.  
                </section>

                <section>
                	<h3>Example: dice roll</h3>
                	\[X \to \{1, ..., 6\}\]
                	\[Y \to \{odd, even\}\]
                	<br/><br/>
                	<span class="fragment fade-in">Before knowing $Y$: $H(X) = \log_2 6$<br/></span>
                	<span class="fragment fade-in">After knowing $Y$: $H(X|Y) = \log_2 3$</span>
                </section>

                <section>
                \[I(X;Y) = H(X) - H(X|Y)\]
                </section>

                <section>
                	<h3>Example: Binary Channel</h3>
                	<br/><br/>
                	In this channel, $X$ takes the value of one with probability $q$, and the error probability is $p$
                	<br/><br/>
                	<span class="fragment fade-in">$I(X;Y) = H(q) - H(p)$</span>
                </section>

                <section>
                	Chosing $X$ to maximize $I(X;Y)$, gives us $I(X;Y) = 0.469$. If we transmit at rates below this, error-free communication is possible.
                	<br/><br/>
                	<span class="fragment fade-in">\[C = \sup(I)\]</span>
                </section>

                <section>
                    <h2>Neuroscience Models using Information Theory</h2>
                </section>

                <section>
                	<h3>Difficulties</h3>
                	Assumptions and differences from engineering systems
                </section>

                <section>
                	<h3>Why is Entropy useful in neuroscience?</h3>
                	The brain <b>compresses</b> information
                </section>

				<section>
                	<h3>Why are Channel Capacity and Mutual Information useful in neuroscience?</h3>
                	Neuronal spikes as depending on input, similar to a <b>noisy channel</b>
                </section>

                <section>
                	<h3>Modulation: Neuronal Spikes and Spike Interval Coding</h3>
                	<span class="fragment fade-in"><img src="images/spikes.png"></span>
                </section>

                <section>
                    <h2>Example: Channel Capacity of a Neuron</h2>
                </section>

                <section>
                	<p><img src="images/tpvsrate.jpg"></p>
                	<br/><br/>
                	\[C_T = 34,68 bps\]
                	\[C_R = 44,95 bps\]
                	<br/><br/>
                	<span class="fragment fade-in">Discrete distribution doesn't reflect the biology of the system</span>
                </section>

                <section>
                	<h3>Optimizing output</h3>
                	<br/><br/>
                	<span class="fragment fade-in">Given the plasticity and evolution of neural systems, this is a reasonable approach</span>
                </section>

                <section>
                	<h3>Discussion and Interpretation</h3>
                </section>

                <section>
                	<h3>Input and Output</h3>
                	<br/><br/>
                	<ul>
	                	<span class="fragment fade-in"><li>Single and memoryless input</li></span><br/>
	                	<span class="fragment fade-in"><li>Less restrictive assumptions</li></span><br/>
	                	<span class="fragment fade-in"><li>Soft and Hard Decoding of $\tilde{X}$</li></span><br/>
                	</ul>
                </section>

                <section>
                	<h3>Discreteness</h3>
                	<br/><br/>
                	<ul>
	                	<span class="fragment fade-in"><li>The capacity is achieved by a discrete probability distribution</li></span><br/>
	                	<span class="fragment fade-in"><li>We believe that this is <b>not</b> true on biological systems, therefore it's <b>working under it's capacity</b></li></span><br/>
	                	<span class="fragment fade-in"><li><b>Feedback</b> is prevalent in many parts of the brain</li></span><br/>
	                	<span class="fragment fade-in"><li></li></span><br/>
	                	<span class="fragment fade-in"><li>Information may be shared by many neurons</li></span><br/>
                	</ul>
                </section>

                <!-- Ending -->
                <section>
                	<h1>Wrapping Up</h1>
                </section>

				<section>
                	<h2>Conclusions</h2>
                </section>          

                <section>
                    <h2>Other works</h2>
                    <ul>
                    	<li>Non-separation of computation and communication on neural systems <i>(Gastpar et al., 2003)</i></li><br/>
                    	<li>Bayesian algorithms during decision making, predictions and pattern recognition <i>(Rao and Ballard, 1999; S. Lee and Mumford, 2003; Knill and Pouget, 2004; George and Hawkins)</i></li><br/>
                    	<li>Analog cortical error correction codes <i>(Fiete et al., 2008)</i></li><br/>
                    	<li>Directed information theory and applications <i>(Granger, 1969; Marko, 1973; Rissanen and Wax, 1987; Massey, 1990; Tatikonda and Mitter, 2009; Hesse et al., 2003; Eichler, 2006; Waddell et al., 2007; Amblard and Michel, 2011; Quinn et al., 2011)</i></li><br/>
                    	<li>Relationship between control, information theory and thermodynamics <i>(Mitter and Newton, 2005; Friston, 2010; Mitter, 2010)</i></li>
                    </ul>
                </section>

                <!-- Refs -->
                <section>
                    <h2>References</h2>
                    <ul>
	                    <li>Destexhe, A., & Contreras, D. (2006). Neuronal computations with stochastic network states. Science (New York, N.Y.), 314(5796), 85–90. doi:10.1126/science.1127241</li>
	                    <li>Dimitrov, A. G., Lazar, A. A., & Victor, J. D. (2011). Information theory in neuroscience. Journal of Computational Neuroscience.</li>
	                    <li>O’Reilly, R. C. (2006). Biologically based computational models of high-level cognition. Science (New York, N.Y.), 314(5796), 91–4. doi:10.1126/science.1127242</li>
	                    <li>McDonnell, M. D., Ikeda, S., & Manton, J. H. (2011). An introductory review of information theory in the context of computational neuroscience. Biological Cybernetics. doi:10.1007/s00422-011-0451-9</li>
	                    <li>Colombo, M., & Series, P. (2012). Bayes in the Brain--On Bayesian Modelling in Neuroscience. The British Journal for the Philosophy of Science, 63, 697–723. doi:10.1093/bjps/axr043</li>
                    </ul>	
                </section>

                <section>
                	<h2>References</h2>
	                <ul>
	                	<li>Wikipedia contributors. "Bayesian approaches to brain function." Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 9 Apr. 2014. Web. 21 Jun. 2014.</li>
	                	<li>Wikipedia contributors. "Computational neuroscience." Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 18 Mar. 2014. Web. 21 Jun. 2014. </li>
	                    <li>Wikipedia contributors. "Information theory." Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 9 Jun. 2014. Web. 21 Jun. 2014.</li>
	                </ul>
                </section>
            </div>
        </div>

        <script src="lib/js/head.min.js"></script>
        <script src="js/reveal.min.js"></script>

        <script>

            // Full list of configuration options available here:
            // https://github.com/hakimel/reveal.js#configuration

            Reveal.initialize({
                controls: false,
                progress: true,
                history: true,
                center: true,
                touch: true,
                autoSlide: 0,
                autoSlideStoppable: true,


                theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
                transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none

                math: {
                  mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
                  config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
                },

                // Optional libraries used to extend on reveal.js
                dependencies: [
                    { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
                    { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
                    { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
                    { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
                    { src: 'plugin/math/math.js', async: true }
                ]
            });

        </script>

    </body>
</html>
